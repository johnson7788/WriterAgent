#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Date  : 2025/9/7 08:06
# @File  : train_thinking.py
# @Author: johnson
# @Contact : github: johnson7788
# @Desc  :
# @Desc  : Qwen3 Thinking è®­ç»ƒï¼ˆå‚è€ƒå®˜æ–¹ notebookï¼‰ï¼Œå¤ç”¨ unsloth_core

from __future__ import annotations
import time
import argparse
import json
import dotenv
import logging
from dataclasses import asdict
from datasets import load_dataset, Dataset
from unsloth_core import (
    clip_display_text,
    TrainConfig,
    setup_logging,
    set_seed,
    log_env_info,
    setup_wandb,
    wandb_on_error,
    wandb_on_success,
    build_model_and_tokenizer,
    build_trainer,
    train_and_report,
    save_model,
)
dotenv.load_dotenv()


def parse_bool_flag(parser: argparse.ArgumentParser, true_flag: str, false_flag: str, default: bool):
    """åŒæ—¶æ”¯æŒ --flag / --no_flag çš„å¸ƒå°”å¼€å…³ã€‚è¿”å›å­˜å…¥ args çš„ç›®æ ‡åã€‚"""
    dest = true_flag.replace("--", "").replace("-", "_")
    group = parser.add_mutually_exclusive_group()
    group.add_argument(true_flag, dest=dest, action="store_true")
    group.add_argument(false_flag, dest=dest, action="store_false")
    parser.set_defaults(**{dest: default})
    return dest


def parse_args() -> TrainConfig:
    parser = argparse.ArgumentParser(description="Thinking SFTï¼ˆunsloth_core é©±åŠ¨ï¼‰")

    # ä¸åŸè„šæœ¬ç­‰ä»·çš„å…³é”®é»˜è®¤å€¼
    parser.add_argument("--model_name", type=str, default="unsloth/Qwen3-4B-Thinking-2507")
    parser.add_argument("--max_seq_length", type=int, default=TrainConfig.max_seq_length)
    parse_bool_flag(parser, "--load_in_4bit", "--no_load_in_4bit", default=TrainConfig.load_in_4bit)
    parse_bool_flag(parser, "--load_in_8bit", "--no_load_in_8bit", default=TrainConfig.load_in_8bit)
    parse_bool_flag(parser, "--full_finetuning", "--no_full_finetuning", default=TrainConfig.full_finetuning)
    parser.add_argument("--hf_token", type=str, default=None)

    parser.add_argument("--lora_r", type=int, default=TrainConfig.lora_r)
    parser.add_argument("--lora_alpha", type=int, default=TrainConfig.lora_alpha)
    parser.add_argument("--lora_dropout", type=float, default=TrainConfig.lora_dropout)

    # Thinking ä¸“ç”¨ chat æ¨¡æ¿
    parser.add_argument("--chat_template", type=str, default="qwen3-thinking")
    parser.add_argument("--dataset_name", type=str, default="unsloth/OpenMathReasoning-mini")
    parser.add_argument("--dataset_split", type=str, default="cot")

    parser.add_argument("--per_device_train_batch_size", type=int, default=TrainConfig.per_device_train_batch_size)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=TrainConfig.gradient_accumulation_steps)
    parser.add_argument("--warmup_steps", type=int, default=TrainConfig.warmup_steps)
    parser.add_argument("--max_steps", type=int, default=TrainConfig.max_steps)
    parser.add_argument("--num_train_epochs", type=float, default=TrainConfig.num_train_epochs)
    parser.add_argument("--learning_rate", type=float, default=TrainConfig.learning_rate)
    parser.add_argument("--logging_steps", type=int, default=TrainConfig.logging_steps)
    parser.add_argument("--optim", type=str, default=TrainConfig.optim)
    parser.add_argument("--weight_decay", type=float, default=TrainConfig.weight_decay)
    parser.add_argument("--lr_scheduler_type", type=str, default=TrainConfig.lr_scheduler_type)
    parser.add_argument("--seed", type=int, default=TrainConfig.seed)
    parser.add_argument("--report_to", type=str, default=TrainConfig.report_to)

    parser.add_argument("--data_files", type=str, default=None,help="é€—å·åˆ†éš”çš„æœ¬åœ°æ•°æ®æ–‡ä»¶ï¼Œå¦‚ data/train.jsonl")

    parser.add_argument("--output_dir", type=str, default="./outputs/qwen3_4b_thinking_lora")
    parser.add_argument("--save_steps", type=int, default=TrainConfig.save_steps)
    parser.add_argument("--save_total_limit", type=int, default=None)

    # W&B ç›¸å…³å¼€å…³
    parse_bool_flag(parser, "--use_wandb", "--no_use_wandb", default=TrainConfig.use_wandb)
    parser.add_argument("--wandb_project", type=str, default="train_thinking")
    parser.add_argument("--wandb_entity", type=str, default=TrainConfig.wandb_entity)
    parser.add_argument("--wandb_run_name", type=str, default=TrainConfig.wandb_run_name)
    parser.add_argument("--wandb_group", type=str, default=TrainConfig.wandb_group)
    parser.add_argument("--wandb_job_type", type=str, default=TrainConfig.wandb_job_type)
    parser.add_argument("--wandb_mode", type=str, default=TrainConfig.wandb_mode)
    parser.add_argument("--wandb_dir", type=str, default=TrainConfig.wandb_dir)
    parser.add_argument("--wandb_notes", type=str, default=TrainConfig.wandb_notes)
    parser.add_argument("--wandb_log_model", type=str, default=str(TrainConfig.wandb_log_model))
    parser.add_argument("--wandb_tags", type=str, nargs="*", default=None, help="ç©ºæ ¼åˆ†éš”çš„ tag åˆ—è¡¨")

    args = parser.parse_args()

    # è§£æ wandb_log_model ä¸º bool/str
    wandb_log_model: bool | str
    if str(args.wandb_log_model).lower() in {"true", "1", "yes"}:
        wandb_log_model = True
    elif str(args.wandb_log_model).lower() in {"false", "0", "no"}:
        wandb_log_model = False
    else:
        wandb_log_model = str(args.wandb_log_model)

    cfg = TrainConfig(
        model_name=args.model_name,
        max_seq_length=args.max_seq_length,
        load_in_4bit=args.load_in_4bit,
        load_in_8bit=args.load_in_8bit,
        full_finetuning=args.full_finetuning,
        hf_token=args.hf_token,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        chat_template=args.chat_template,
        dataset_name=args.dataset_name,
        dataset_split=args.dataset_split,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        warmup_steps=args.warmup_steps,
        max_steps=args.max_steps,
        num_train_epochs=args.num_train_epochs,
        learning_rate=args.learning_rate,
        logging_steps=args.logging_steps,
        optim=args.optim,
        weight_decay=args.weight_decay,
        lr_scheduler_type=args.lr_scheduler_type,
        seed=args.seed,
        report_to=args.report_to,
        data_files=args.data_files,
        output_dir=args.output_dir,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        use_wandb=args.use_wandb,
        wandb_project=args.wandb_project,
        wandb_entity=args.wandb_entity,
        wandb_run_name=args.wandb_run_name,
        wandb_tags=args.wandb_tags or TrainConfig().wandb_tags,
        wandb_dir=args.wandb_dir,
        wandb_group=args.wandb_group,
        wandb_job_type=args.wandb_job_type,
        wandb_mode=args.wandb_mode,
        wandb_notes=args.wandb_notes,
        wandb_log_model=wandb_log_model,
    )
    # Thinking æ¨¡å‹åŒæ ·ä½¿ç”¨ responses-only çš„åˆ†éš”ç¬¦ï¼ˆä¸åŸå§‹è„šæœ¬ä¸€è‡´ï¼‰
    cfg.instruction_part = "<|im_start|>user\n"
    cfg.response_part = "<|im_start|>assistant\n"
    cfg.dataset_text_field = "text"
    return cfg

def _safe_json_dumps(obj) -> str:
    """å°½é‡å®Œæ•´åœ°æ‰“å°æ ·æœ¬ï¼Œé¿å… numpy/int64 ç­‰ç±»å‹å¯¼è‡´çš„åºåˆ—åŒ–æŠ¥é”™"""
    def _default(o):
        try:
            import numpy as np
            if isinstance(o, (np.integer,)):
                return int(o)
            if isinstance(o, (np.floating,)):
                return float(o)
        except Exception:
            pass
        return str(o)
    return json.dumps(obj, ensure_ascii=False, indent=2, default=_default)


def prepare_dataset_openmath_thinking(cfg: TrainConfig, tokenizer, logger: logging.Logger, *, split: str = "cot") -> Dataset:
    """OpenMathReasoning-mini â†’ conversations â†’ text"""
    t0 = time.perf_counter()
    logger.info("åŠ è½½æ•°æ®é›†: %s [%s] â€¦", cfg.dataset_name, cfg.dataset_split)
    ds = load_dataset(cfg.dataset_name, split=cfg.dataset_split)
    logger.info("åŠ è½½å®Œæˆ: %d è¡Œï¼Œå­—æ®µ: %s", len(ds), ds.column_names)

    # è½»é‡æ ¡éªŒ
    for col in ("problem", "generated_solution"):
        if col not in ds.column_names:
            raise KeyError(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {col}")

    # ç®€çŸ­é¢„è§ˆï¼ˆåŸå§‹ï¼‰
    if len(ds) > 0:
        logger.info("åŸå§‹æ ·æœ¬#0.problem: %s", clip_display_text(ds[0]["problem"]))
        logger.info("åŸå§‹æ ·æœ¬#0.solution: %s", clip_display_text(ds[0]["generated_solution"]))

    # æ„é€  conversations
    def _to_conversations(batch):
        convs = []
        for p, s in zip(batch["problem"], batch["generated_solution"]):
            if isinstance(p, str) and isinstance(s, str) and p.strip() and s.strip():
                convs.append([{"role": "user", "content": p.strip()},
                              {"role": "assistant", "content": s.strip()}])
        return {"conversations": convs}

    t1 = time.perf_counter()
    ds = ds.map(_to_conversations, batched=True, desc="build_conversations")
    logger.info("build_conversations å®Œæˆï¼ˆç”¨æ—¶ %.2fsï¼‰", time.perf_counter() - t1)

    # é¢„è§ˆï¼ˆå¯¹è¯ï¼‰
    if len(ds) > 0 and "conversations" in ds.column_names:
        c0 = ds[0]["conversations"]
        logger.info("å¯¹è¯æ ·æœ¬#0.user: %s", clip_display_text(c0[0]["content"]))
        logger.info("å¯¹è¯æ ·æœ¬#0.assistant: %s", clip_display_text(c0[1]["content"]))

    # åº”ç”¨ chat æ¨¡æ¿
    def _format(batch):
        texts = [tokenizer.apply_chat_template(c, tokenize=False, add_generation_prompt=False)
                 for c in batch["conversations"]]
        return {"text": texts}

    t2 = time.perf_counter()
    ds = ds.map(_format, batched=True, desc="apply_chat_template")
    logger.info("apply_chat_template å®Œæˆï¼ˆç”¨æ—¶ %.2fsï¼‰", time.perf_counter() - t2)

    # é¢„è§ˆï¼ˆæœ€ç»ˆæ–‡æœ¬ï¼‰
    if len(ds) > 0:
        if "text" in ds.column_names:
            logger.info("æ¨¡æ¿åŒ–æ ·æœ¬#0.text(æˆªæ–­é¢„è§ˆ): %s", clip_display_text(ds[0]["text"]))
        try:
            logger.info("==== æ ·æœ¬#0 å®Œæ•´è®°å½•ï¼ˆOpenMath pipeline ä¹‹åï¼‰ ====\n%s", _safe_json_dumps(ds[0]))
        except Exception as e:
            logger.warning("æ‰“å°å®Œæ•´æ ·æœ¬å¤±è´¥: %r", e)
    logger.info("æ•°æ®é›†å‡†å¤‡å®Œæˆï¼š%d è¡Œï¼Œæ€»è€—æ—¶ %.2fs", len(ds), time.perf_counter() - t0)
    return ds

def _parse_data_files_arg(s: str) -> dict | list | str:
    """
    å°† --data_files çš„å­—ç¬¦ä¸²è§£æä¸º datasets.load_dataset(data_files=...) æ¥å—çš„å½¢å¼ã€‚
    æ”¯æŒï¼š
      - "data/train.jsonl"
      - "train=data/train-*.jsonl"
      - "train=... , validation=..."
      - "a.jsonl,b.jsonl"ï¼ˆç­‰ä»·äºå•ä¸€ 'train' æ‹†åˆ†çš„å¤šæ–‡ä»¶åˆ—è¡¨ï¼‰
    """
    s = (s or "").strip()
    if not s:
        return s
    parts = [p.strip() for p in s.split(",") if p.strip()]
    if all("=" in p for p in parts):
        out = {}
        for p in parts:
            k, v = p.split("=", 1)
            out[k.strip()] = v.strip()
        return out  # e.g. {"train":"...", "validation":"..."}
    if len(parts) == 1:
        return parts[0]        # å•æ–‡ä»¶
    return parts               # å¤šæ–‡ä»¶åˆ—è¡¨ â†’ å•ä¸€æ‹†åˆ†

def prepare_dataset_from_json_files(cfg: TrainConfig, tokenizer, logger: logging.Logger) -> Dataset:
    """
    é€šç”¨è‡ªå®šä¹‰æ•°æ®é›†åŠ è½½ï¼š
      - æ¥å— conversations/messages/input+output å…¶ä¸­ä¸€ç§
      - ç»Ÿä¸€ç”Ÿæˆ 'text' åˆ—ä¾› SFT ä½¿ç”¨
    """
    assert cfg.data_files, "cfg.data_files ä¸èƒ½ä¸ºç©º"
    df_arg = _parse_data_files_arg(cfg.data_files)

    logger.info("ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®: %s", df_arg)
    raw = load_dataset("json", data_files=df_arg)

    # é€‰æ‹©æ‹†åˆ†ï¼šä¼˜å…ˆ cfg.dataset_splitï¼›å¦åˆ™å–ç¬¬ä¸€ä¸ªå¯ç”¨é”®ï¼›è‹¥ load_dataset è¿”å› Dataset åˆ™ç›´æ¥ç”¨
    if isinstance(raw, dict) or hasattr(raw, "keys"):  # DatasetDict
        keys = list(raw.keys())
        split = cfg.dataset_split if cfg.dataset_split in keys else (keys[0] if keys else "train")
        ds = raw[split]
        logger.info("å·²é€‰æ‹©æ‹†åˆ†: %sï¼ˆå¯ç”¨: %sï¼‰", split, keys)
    else:
        ds = raw

    cols = set(ds.column_names)
    logger.info("åŸå§‹å­—æ®µ: %s", cols)

    # é¢„å¤„ç†ä¸º conversations
    if "conversations" in cols:
        conv_field = "conversations"
    elif "messages" in cols:
        conv_field = "messages"
    elif {"input", "output"}.issubset(cols):
        conv_field = None
        def to_conv_io(batch):
            convs = []
            for x, y in zip(batch["input"], batch["output"]):
                if isinstance(x, str) and isinstance(y, str) and x.strip() and y.strip():
                    convs.append([{"role": "user", "content": x.strip()},
                                  {"role": "assistant", "content": y.strip()}])
            return {"conversations": convs}
        ds = ds.map(to_conv_io, batched=True, desc="build_conversations_from_io")
        conv_field = "conversations"
    elif "text" in cols:
        # å·²ç»æ˜¯æœ€ç»ˆæ–‡æœ¬ï¼Œç›´æ¥è¿”å›
        logger.info("æ£€æµ‹åˆ°å·²æœ‰ 'text' å­—æ®µï¼Œè·³è¿‡æ¨¡æ¿åŒ–ã€‚")
        return ds
    else:
        raise KeyError("æœªæ£€æµ‹åˆ°æ”¯æŒçš„æ•°æ®æ ¼å¼ã€‚éœ€è¦ 'conversations'/'messages' æˆ– 'input'+'output'ï¼Œ"
                       "æˆ–ç›´æ¥æä¾› 'text'ã€‚")

    # åº”ç”¨ chat æ¨¡æ¿ â†’ ç”Ÿæˆ text
    def _format(batch):
        texts = [tokenizer.apply_chat_template(c, tokenize=False, add_generation_prompt=False)
                 for c in batch[conv_field]]
        return {"text": texts}
    ds = ds.map(_format, batched=True, desc="apply_chat_template")
    if len(ds) > 0:
        try:
            logger.info("==== æ ·æœ¬#0 å®Œæ•´è®°å½•ï¼ˆè‡ªå®šä¹‰æ•°æ® pipeline ä¹‹åï¼‰ ====\n%s",_safe_json_dumps(ds[0]))
        except Exception as e:
            logger.warning("æ‰“å°å®Œæ•´æ ·æœ¬å¤±è´¥: %r", e)
    logger.info("è‡ªå®šä¹‰æ•°æ®é›†å‡†å¤‡å®Œæˆï¼š%d è¡Œã€‚", len(ds))
    return ds

def main(cfg: TrainConfig | None = None) -> None:
    cfg = cfg or TrainConfig()

    # æ—¥å¿—
    logger = setup_logging(cfg.output_dir, cfg.logging_dir)

    # æ‰“å°é…ç½®
    logger.info("===== è®­ç»ƒé…ç½® =====")
    logger.info(json.dumps(asdict(cfg), ensure_ascii=False, indent=2))

    # éšæœºç§å­ & ç¯å¢ƒä¿¡æ¯
    set_seed(cfg.seed, logger)
    log_env_info(logger)

    # åˆå§‹åŒ– W&Bï¼ˆå°½æ—©å»ºç«‹ runï¼Œè®°å½•ç¯å¢ƒ/é…ç½®ï¼‰
    run = setup_wandb(cfg, logger)

    # æ„å»ºæ¨¡å‹ä¸ tokenizer
    model, tokenizer = build_model_and_tokenizer(cfg, logger)

    # å‡†å¤‡æ•°æ®é›†ï¼šä¼˜å…ˆä½¿ç”¨æœ¬åœ° data_filesï¼›å¦åˆ™ç”¨ OpenMathReasoning

    if cfg.data_files:
        dataset = prepare_dataset_from_json_files(cfg, tokenizer, logger)
    else:
        dataset = prepare_dataset_openmath_thinking(cfg, tokenizer, logger)
    # æ„å»º Trainer
    trainer = build_trainer(model, tokenizer, dataset, cfg, logger)

    # è®­ç»ƒå¹¶æŠ¥å‘Š
    stats = train_and_report(trainer, logger)

    # ä¿å­˜æ¨¡å‹ & å¯é€‰ä¸Šä¼  artifactï¼ˆåˆ«åï¼šfinalï¼‰
    save_model(
        trainer,
        tokenizer,
        cfg.output_dir,
        logger,
        log_artifact=(cfg.wandb_log_model if cfg.wandb_log_model else False),
    )

    # æˆåŠŸæ”¶å°¾
    extra = {"metrics/train_runtime_sec": float(stats.metrics.get("train_runtime", 0.0))}
    wandb_on_success(extra_summary=extra, exit_code=0)

    logger.info(f"{float(stats.metrics.get('train_runtime', 0.0)):.2f} ç§’ used for training.")
    logger.info("ğŸ‰  è®­ç»ƒç»“æŸã€‚")

if __name__ == "__main__":
    main(parse_args())
